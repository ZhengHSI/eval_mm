[2024-10-30 04:53:26 root] (minicpm_v.py 584): INFO Arguments: 
[2024-10-30 04:53:26 root] (minicpm_v.py 584): INFO Arguments: 
[2024-10-30 04:53:26 root] (minicpm_v.py 584): INFO Arguments: 
[2024-10-30 04:53:26 root] (minicpm_v.py 584): INFO Arguments: 
[2024-10-30 04:53:26 root] (minicpm_v.py 584): INFO Arguments: 
[2024-10-30 04:53:26 root] (minicpm_v.py 584): INFO Arguments: 
[2024-10-30 04:53:26 root] (minicpm_v.py 584): INFO Arguments: 
[2024-10-30 04:53:26 root] (minicpm_v.py 584): INFO Arguments: 
[2024-10-30 04:53:26 root] (minicpm_v.py 585): INFO {'a_asym': False,
 'a_bits': 4,
 'a_groupsize': -1,
 'act_order': False,
 'add_diag': True,
 'cache_dir': './outputs/.cache',
 'cali_bsz': 4,
 'cali_dataset': 'wikitext2',
 'cali_trans': True,
 'deactive_amp': False,
 'diag_alpha': 0.3,
 'diag_init': 'sq_style',
 'direct_inv': False,
 'distribute_model': True,
 'epochs': 15,
 'exp_dir': './outputs/MiniCPM-Llama3-V-2_5/w4a4/exp',
 'exp_name': 'exp',
 'flat_lr': 0.005,
 'gptq': False,
 'gptq_mse': False,
 'hf_token': None,
 'k_asym': True,
 'k_bits': 4,
 'k_groupsize': 128,
 'lac': True,
 'lm_eval': False,
 'lm_eval_batch_size': 128,
 'lwc': True,
 'matrix_path': '/home/workspace/code/git/FlatQuant_mlm/outputs/MiniCPM-Llama3-V-2_5/w4a4/exp-mme/',
 'model': '/home/workspace/model/MiniCPM-Llama3-V-2_5',
 'model_name': 'MiniCPM-Llama3-V-2_5',
 'nsamples': 128,
 'output_dir': './outputs',
 'percdamp': 0.01,
 'q_asym': False,
 'q_bits': 16,
 'q_groupsize': -1,
 'reload_matrix': True,
 'resume': False,
 'save_matrix': True,
 'seed': 0,
 'separate_vtrans': False,
 'tasks': ['piqa',
           'hellaswag',
           'arc_easy',
           'arc_challenge',
           'winogrande',
           'lambada_openai'],
 'v_asym': True,
 'v_bits': 4,
 'v_groupsize': 128,
 'w_asym': False,
 'w_bits': 4,
 'w_groupsize': -1,
 'warmup': False}
[2024-10-30 04:53:26 root] (minicpm_v.py 585): INFO {'a_asym': False,
 'a_bits': 4,
 'a_groupsize': -1,
 'act_order': False,
 'add_diag': True,
 'cache_dir': './outputs/.cache',
 'cali_bsz': 4,
 'cali_dataset': 'wikitext2',
 'cali_trans': True,
 'deactive_amp': False,
 'diag_alpha': 0.3,
 'diag_init': 'sq_style',
 'direct_inv': False,
 'distribute_model': True,
 'epochs': 15,
 'exp_dir': './outputs/MiniCPM-Llama3-V-2_5/w4a4/exp',
 'exp_name': 'exp',
 'flat_lr': 0.005,
 'gptq': False,
 'gptq_mse': False,
 'hf_token': None,
 'k_asym': True,
 'k_bits': 4,
 'k_groupsize': 128,
 'lac': True,
 'lm_eval': False,
 'lm_eval_batch_size': 128,
 'lwc': True,
 'matrix_path': '/home/workspace/code/git/FlatQuant_mlm/outputs/MiniCPM-Llama3-V-2_5/w4a4/exp-mme/',
 'model': '/home/workspace/model/MiniCPM-Llama3-V-2_5',
 'model_name': 'MiniCPM-Llama3-V-2_5',
 'nsamples': 128,
 'output_dir': './outputs',
 'percdamp': 0.01,
 'q_asym': False,
 'q_bits': 16,
 'q_groupsize': -1,
 'reload_matrix': True,
 'resume': False,
 'save_matrix': True,
 'seed': 0,
 'separate_vtrans': False,
 'tasks': ['piqa',
           'hellaswag',
           'arc_easy',
           'arc_challenge',
           'winogrande',
           'lambada_openai'],
 'v_asym': True,
 'v_bits': 4,
 'v_groupsize': 128,
 'w_asym': False,
 'w_bits': 4,
 'w_groupsize': -1,
 'warmup': False}
[2024-10-30 04:53:26 root] (minicpm_v.py 585): INFO {'a_asym': False,
 'a_bits': 4,
 'a_groupsize': -1,
 'act_order': False,
 'add_diag': True,
 'cache_dir': './outputs/.cache',
 'cali_bsz': 4,
 'cali_dataset': 'wikitext2',
 'cali_trans': True,
 'deactive_amp': False,
 'diag_alpha': 0.3,
 'diag_init': 'sq_style',
 'direct_inv': False,
 'distribute_model': True,
 'epochs': 15,
 'exp_dir': './outputs/MiniCPM-Llama3-V-2_5/w4a4/exp',
 'exp_name': 'exp',
 'flat_lr': 0.005,
 'gptq': False,
 'gptq_mse': False,
 'hf_token': None,
 'k_asym': True,
 'k_bits': 4,
 'k_groupsize': 128,
 'lac': True,
 'lm_eval': False,
 'lm_eval_batch_size': 128,
 'lwc': True,
 'matrix_path': '/home/workspace/code/git/FlatQuant_mlm/outputs/MiniCPM-Llama3-V-2_5/w4a4/exp-mme/',
 'model': '/home/workspace/model/MiniCPM-Llama3-V-2_5',
 'model_name': 'MiniCPM-Llama3-V-2_5',
 'nsamples': 128,
 'output_dir': './outputs',
 'percdamp': 0.01,
 'q_asym': False,
 'q_bits': 16,
 'q_groupsize': -1,
 'reload_matrix': True,
 'resume': False,
 'save_matrix': True,
 'seed': 0,
 'separate_vtrans': False,
 'tasks': ['piqa',
           'hellaswag',
           'arc_easy',
           'arc_challenge',
           'winogrande',
           'lambada_openai'],
 'v_asym': True,
 'v_bits': 4,
 'v_groupsize': 128,
 'w_asym': False,
 'w_bits': 4,
 'w_groupsize': -1,
 'warmup': False}
[2024-10-30 04:53:26 root] (minicpm_v.py 585): INFO {'a_asym': False,
 'a_bits': 4,
 'a_groupsize': -1,
 'act_order': False,
 'add_diag': True,
 'cache_dir': './outputs/.cache',
 'cali_bsz': 4,
 'cali_dataset': 'wikitext2',
 'cali_trans': True,
 'deactive_amp': False,
 'diag_alpha': 0.3,
 'diag_init': 'sq_style',
 'direct_inv': False,
 'distribute_model': True,
 'epochs': 15,
 'exp_dir': './outputs/MiniCPM-Llama3-V-2_5/w4a4/exp',
 'exp_name': 'exp',
 'flat_lr': 0.005,
 'gptq': False,
 'gptq_mse': False,
 'hf_token': None,
 'k_asym': True,
 'k_bits': 4,
 'k_groupsize': 128,
 'lac': True,
 'lm_eval': False,
 'lm_eval_batch_size': 128,
 'lwc': True,
 'matrix_path': '/home/workspace/code/git/FlatQuant_mlm/outputs/MiniCPM-Llama3-V-2_5/w4a4/exp-mme/',
 'model': '/home/workspace/model/MiniCPM-Llama3-V-2_5',
 'model_name': 'MiniCPM-Llama3-V-2_5',
 'nsamples': 128,
 'output_dir': './outputs',
 'percdamp': 0.01,
 'q_asym': False,
 'q_bits': 16,
 'q_groupsize': -1,
 'reload_matrix': True,
 'resume': False,
 'save_matrix': True,
 'seed': 0,
 'separate_vtrans': False,
 'tasks': ['piqa',
           'hellaswag',
           'arc_easy',
           'arc_challenge',
           'winogrande',
           'lambada_openai'],
 'v_asym': True,
 'v_bits': 4,
 'v_groupsize': 128,
 'w_asym': False,
 'w_bits': 4,
 'w_groupsize': -1,
 'warmup': False}
[2024-10-30 04:53:26 root] (minicpm_v.py 585): INFO {'a_asym': False,
 'a_bits': 4,
 'a_groupsize': -1,
 'act_order': False,
 'add_diag': True,
 'cache_dir': './outputs/.cache',
 'cali_bsz': 4,
 'cali_dataset': 'wikitext2',
 'cali_trans': True,
 'deactive_amp': False,
 'diag_alpha': 0.3,
 'diag_init': 'sq_style',
 'direct_inv': False,
 'distribute_model': True,
 'epochs': 15,
 'exp_dir': './outputs/MiniCPM-Llama3-V-2_5/w4a4/exp',
 'exp_name': 'exp',
 'flat_lr': 0.005,
 'gptq': False,
 'gptq_mse': False,
 'hf_token': None,
 'k_asym': True,
 'k_bits': 4,
 'k_groupsize': 128,
 'lac': True,
 'lm_eval': False,
 'lm_eval_batch_size': 128,
 'lwc': True,
 'matrix_path': '/home/workspace/code/git/FlatQuant_mlm/outputs/MiniCPM-Llama3-V-2_5/w4a4/exp-mme/',
 'model': '/home/workspace/model/MiniCPM-Llama3-V-2_5',
 'model_name': 'MiniCPM-Llama3-V-2_5',
 'nsamples': 128,
 'output_dir': './outputs',
 'percdamp': 0.01,
 'q_asym': False,
 'q_bits': 16,
 'q_groupsize': -1,
 'reload_matrix': True,
 'resume': False,
 'save_matrix': True,
 'seed': 0,
 'separate_vtrans': False,
 'tasks': ['piqa',
           'hellaswag',
           'arc_easy',
           'arc_challenge',
           'winogrande',
           'lambada_openai'],
 'v_asym': True,
 'v_bits': 4,
 'v_groupsize': 128,
 'w_asym': False,
 'w_bits': 4,
 'w_groupsize': -1,
 'warmup': False}
[2024-10-30 04:53:26 root] (minicpm_v.py 585): INFO {'a_asym': False,
 'a_bits': 4,
 'a_groupsize': -1,
 'act_order': False,
 'add_diag': True,
 'cache_dir': './outputs/.cache',
 'cali_bsz': 4,
 'cali_dataset': 'wikitext2',
 'cali_trans': True,
 'deactive_amp': False,
 'diag_alpha': 0.3,
 'diag_init': 'sq_style',
 'direct_inv': False,
 'distribute_model': True,
 'epochs': 15,
 'exp_dir': './outputs/MiniCPM-Llama3-V-2_5/w4a4/exp',
 'exp_name': 'exp',
 'flat_lr': 0.005,
 'gptq': False,
 'gptq_mse': False,
 'hf_token': None,
 'k_asym': True,
 'k_bits': 4,
 'k_groupsize': 128,
 'lac': True,
 'lm_eval': False,
 'lm_eval_batch_size': 128,
 'lwc': True,
 'matrix_path': '/home/workspace/code/git/FlatQuant_mlm/outputs/MiniCPM-Llama3-V-2_5/w4a4/exp-mme/',
 'model': '/home/workspace/model/MiniCPM-Llama3-V-2_5',
 'model_name': 'MiniCPM-Llama3-V-2_5',
 'nsamples': 128,
 'output_dir': './outputs',
 'percdamp': 0.01,
 'q_asym': False,
 'q_bits': 16,
 'q_groupsize': -1,
 'reload_matrix': True,
 'resume': False,
 'save_matrix': True,
 'seed': 0,
 'separate_vtrans': False,
 'tasks': ['piqa',
           'hellaswag',
           'arc_easy',
           'arc_challenge',
           'winogrande',
           'lambada_openai'],
 'v_asym': True,
 'v_bits': 4,
 'v_groupsize': 128,
 'w_asym': False,
 'w_bits': 4,
 'w_groupsize': -1,
 'warmup': False}
[2024-10-30 04:53:26 root] (minicpm_v.py 585): INFO {'a_asym': False,
 'a_bits': 4,
 'a_groupsize': -1,
 'act_order': False,
 'add_diag': True,
 'cache_dir': './outputs/.cache',
 'cali_bsz': 4,
 'cali_dataset': 'wikitext2',
 'cali_trans': True,
 'deactive_amp': False,
 'diag_alpha': 0.3,
 'diag_init': 'sq_style',
 'direct_inv': False,
 'distribute_model': True,
 'epochs': 15,
 'exp_dir': './outputs/MiniCPM-Llama3-V-2_5/w4a4/exp',
 'exp_name': 'exp',
 'flat_lr': 0.005,
 'gptq': False,
 'gptq_mse': False,
 'hf_token': None,
 'k_asym': True,
 'k_bits': 4,
 'k_groupsize': 128,
 'lac': True,
 'lm_eval': False,
 'lm_eval_batch_size': 128,
 'lwc': True,
 'matrix_path': '/home/workspace/code/git/FlatQuant_mlm/outputs/MiniCPM-Llama3-V-2_5/w4a4/exp-mme/',
 'model': '/home/workspace/model/MiniCPM-Llama3-V-2_5',
 'model_name': 'MiniCPM-Llama3-V-2_5',
 'nsamples': 128,
 'output_dir': './outputs',
 'percdamp': 0.01,
 'q_asym': False,
 'q_bits': 16,
 'q_groupsize': -1,
 'reload_matrix': True,
 'resume': False,
 'save_matrix': True,
 'seed': 0,
 'separate_vtrans': False,
 'tasks': ['piqa',
           'hellaswag',
           'arc_easy',
           'arc_challenge',
           'winogrande',
           'lambada_openai'],
 'v_asym': True,
 'v_bits': 4,
 'v_groupsize': 128,
 'w_asym': False,
 'w_bits': 4,
 'w_groupsize': -1,
 'warmup': False}
[2024-10-30 04:53:26 root] (minicpm_v.py 585): INFO {'a_asym': False,
 'a_bits': 4,
 'a_groupsize': -1,
 'act_order': False,
 'add_diag': True,
 'cache_dir': './outputs/.cache',
 'cali_bsz': 4,
 'cali_dataset': 'wikitext2',
 'cali_trans': True,
 'deactive_amp': False,
 'diag_alpha': 0.3,
 'diag_init': 'sq_style',
 'direct_inv': False,
 'distribute_model': True,
 'epochs': 15,
 'exp_dir': './outputs/MiniCPM-Llama3-V-2_5/w4a4/exp',
 'exp_name': 'exp',
 'flat_lr': 0.005,
 'gptq': False,
 'gptq_mse': False,
 'hf_token': None,
 'k_asym': True,
 'k_bits': 4,
 'k_groupsize': 128,
 'lac': True,
 'lm_eval': False,
 'lm_eval_batch_size': 128,
 'lwc': True,
 'matrix_path': '/home/workspace/code/git/FlatQuant_mlm/outputs/MiniCPM-Llama3-V-2_5/w4a4/exp-mme/',
 'model': '/home/workspace/model/MiniCPM-Llama3-V-2_5',
 'model_name': 'MiniCPM-Llama3-V-2_5',
 'nsamples': 128,
 'output_dir': './outputs',
 'percdamp': 0.01,
 'q_asym': False,
 'q_bits': 16,
 'q_groupsize': -1,
 'reload_matrix': True,
 'resume': False,
 'save_matrix': True,
 'seed': 0,
 'separate_vtrans': False,
 'tasks': ['piqa',
           'hellaswag',
           'arc_easy',
           'arc_challenge',
           'winogrande',
           'lambada_openai'],
 'v_asym': True,
 'v_bits': 4,
 'v_groupsize': 128,
 'w_asym': False,
 'w_bits': 4,
 'w_groupsize': -1,
 'warmup': False}
[2024-10-30 04:53:26 root] (minicpm_v.py 586): INFO ------------------------------------------------------------
[2024-10-30 04:53:26 root] (minicpm_v.py 586): INFO ------------------------------------------------------------
[2024-10-30 04:53:26 root] (minicpm_v.py 586): INFO ------------------------------------------------------------
[2024-10-30 04:53:26 root] (minicpm_v.py 586): INFO ------------------------------------------------------------
[2024-10-30 04:53:26 root] (minicpm_v.py 586): INFO ------------------------------------------------------------
[2024-10-30 04:53:26 root] (minicpm_v.py 586): INFO ------------------------------------------------------------
[2024-10-30 04:53:26 root] (minicpm_v.py 586): INFO ------------------------------------------------------------
[2024-10-30 04:53:26 root] (minicpm_v.py 586): INFO ------------------------------------------------------------
[2024-10-30 04:53:26 transformers_modules.MiniCPM-Llama3-V-2_5.configuration_minicpm] (configuration_minicpm.py 105): INFO vision_config is None, using default vision config
[2024-10-30 04:53:26 transformers_modules.MiniCPM-Llama3-V-2_5.configuration_minicpm] (configuration_minicpm.py 105): INFO vision_config is None, using default vision config
[2024-10-30 04:53:26 transformers_modules.MiniCPM-Llama3-V-2_5.configuration_minicpm] (configuration_minicpm.py 105): INFO vision_config is None, using default vision config
[2024-10-30 04:53:26 transformers_modules.MiniCPM-Llama3-V-2_5.configuration_minicpm] (configuration_minicpm.py 105): INFO vision_config is None, using default vision config
[2024-10-30 04:53:26 transformers_modules.MiniCPM-Llama3-V-2_5.configuration_minicpm] (configuration_minicpm.py 105): INFO vision_config is None, using default vision config
[2024-10-30 04:53:26 transformers_modules.MiniCPM-Llama3-V-2_5.configuration_minicpm] (configuration_minicpm.py 105): INFO vision_config is None, using default vision config
[2024-10-30 04:53:26 transformers_modules.MiniCPM-Llama3-V-2_5.configuration_minicpm] (configuration_minicpm.py 105): INFO vision_config is None, using default vision config
[2024-10-30 04:53:26 transformers_modules.MiniCPM-Llama3-V-2_5.configuration_minicpm] (configuration_minicpm.py 105): INFO vision_config is None, using default vision config
[2024-10-30 04:57:59 root] (model_utils.py 83): INFO ---> Loading /home/workspace/model/MiniCPM-Llama3-V-2_5 Model with seq_len: 2048
[2024-10-30 04:57:59 root] (model_utils.py 83): INFO ---> Loading /home/workspace/model/MiniCPM-Llama3-V-2_5 Model with seq_len: 2048
[2024-10-30 04:57:59 root] (model_utils.py 83): INFO ---> Loading /home/workspace/model/MiniCPM-Llama3-V-2_5 Model with seq_len: 2048
[2024-10-30 04:57:59 root] (model_utils.py 83): INFO ---> Loading /home/workspace/model/MiniCPM-Llama3-V-2_5 Model with seq_len: 2048
[2024-10-30 04:57:59 root] (model_utils.py 83): INFO ---> Loading /home/workspace/model/MiniCPM-Llama3-V-2_5 Model with seq_len: 2048
[2024-10-30 04:57:59 root] (model_utils.py 83): INFO ---> Loading /home/workspace/model/MiniCPM-Llama3-V-2_5 Model with seq_len: 2048
[2024-10-30 04:57:59 root] (model_utils.py 83): INFO ---> Loading /home/workspace/model/MiniCPM-Llama3-V-2_5 Model with seq_len: 2048
[2024-10-30 04:57:59 root] (model_utils.py 83): INFO ---> Loading /home/workspace/model/MiniCPM-Llama3-V-2_5 Model with seq_len: 2048
[2024-10-30 04:58:38 root] (minicpm_v.py 594): INFO Finished applying FlatQuant to model.
[2024-10-30 04:58:39 root] (minicpm_v.py 594): INFO Finished applying FlatQuant to model.
[2024-10-30 04:58:40 root] (minicpm_v.py 594): INFO Finished applying FlatQuant to model.
[2024-10-30 04:58:40 root] (minicpm_v.py 594): INFO Finished applying FlatQuant to model.
[2024-10-30 04:58:41 root] (minicpm_v.py 594): INFO Finished applying FlatQuant to model.
[2024-10-30 04:58:41 root] (minicpm_v.py 594): INFO Finished applying FlatQuant to model.
[2024-10-30 04:58:41 root] (minicpm_v.py 594): INFO Finished applying FlatQuant to model.
[2024-10-30 04:58:45 root] (minicpm_v.py 594): INFO Finished applying FlatQuant to model.
[2024-10-30 05:00:40 root] (minicpm_v.py 597): INFO Finished reparameterize model.
[2024-10-30 05:00:41 root] (minicpm_v.py 597): INFO Finished reparameterize model.
[2024-10-30 05:00:42 root] (minicpm_v.py 597): INFO Finished reparameterize model.
[2024-10-30 05:00:43 root] (minicpm_v.py 597): INFO Finished reparameterize model.
[2024-10-30 05:00:43 root] (minicpm_v.py 597): INFO Finished reparameterize model.
[2024-10-30 05:00:46 root] (minicpm_v.py 597): INFO Finished reparameterize model.
[2024-10-30 05:00:47 root] (minicpm_v.py 597): INFO Finished reparameterize model.
[2024-10-30 05:00:53 root] (utils.py 48): INFO GPU memory (from rtn_fwrd): 0.11 -> 0.11 GB (0.00 GB)
[2024-10-30 05:00:54 root] (utils.py 48): INFO GPU memory (from rtn_fwrd): 0.11 -> 0.11 GB (0.00 GB)
[2024-10-30 05:00:55 root] (utils.py 48): INFO GPU memory (from rtn_fwrd): 0.11 -> 0.11 GB (0.00 GB)
[2024-10-30 05:00:55 root] (minicpm_v.py 597): INFO Finished reparameterize model.
[2024-10-30 05:00:58 root] (utils.py 48): INFO GPU memory (from rtn_fwrd): 0.11 -> 0.11 GB (0.00 GB)
[2024-10-30 05:00:58 root] (utils.py 48): INFO GPU memory (from rtn_fwrd): 0.11 -> 0.11 GB (0.00 GB)
[2024-10-30 05:00:59 root] (utils.py 48): INFO GPU memory (from rtn_fwrd): 0.11 -> 0.11 GB (0.00 GB)
[2024-10-30 05:01:01 root] (utils.py 48): INFO GPU memory (from rtn_fwrd): 0.11 -> 0.11 GB (0.00 GB)
[2024-10-30 05:01:05 root] (utils.py 48): INFO GPU memory (from rtn_fwrd): 0.11 -> 0.11 GB (0.00 GB)
[2024-10-30 05:07:29 LOAD_ENV] (misc.py 173): INFO API Keys successfully loaded from /home/workspace/code/git/eval_mm/vlmevalkit/.env
[2024-10-30 05:07:29 ChatAPI] (gpt.py 114): INFO Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
[2024-10-30 05:07:29 ChatAPI] (gpt.py 129): INFO Using API Base: http://15.204.101.64:4000/v1/chat/completions; API Key: sk-5DjKHIAjndzkjCtb85D57d2c3e13403e95B5D2F02c61561b
[2024-10-30 05:07:37 RUN] (run.py 208): INFO The evaluation of model MiniCPM-Llama3-V-2_5-flatquant x dataset MME has finished! 
[2024-10-30 05:07:37 RUN] (run.py 209): INFO Evaluation Results:
[2024-10-30 05:07:37 RUN] (run.py 215): INFO 
---------------------  --------
perception             1523.87
reasoning               362.5
OCR                     125
artwork                 131.5
celebrity               118.235
code_reasoning           45
color                   170
commonsense_reasoning   125
count                   170
existence               195
landmark                162.75
numerical_calculation    52.5
position                141.667
posters                 152.721
scene                   157
text_translation        140
---------------------  --------
